\documentclass[11pt,letterpaper]{article}
\usepackage[utf8]{inputenc}

%----- Configuración del estilo del documento------%
\usepackage{epsfig,graphicx}
\usepackage[left=2cm,right=2cm,top=1.8cm,bottom=2.3cm]{geometry}
\usepackage{fancyhdr}
\usepackage{lastpage}
\pagestyle{fancy}
\fancyhf{}

\usepackage{amsmath,amsthm,amssymb, tikz}
\usepackage{graphicx}
%%Use this package for matrices
\usepackage{array}
\usepackage{breqn}
\usepackage{multicol}
\usepackage{lipsum}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{booktabs}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{float}
\usepackage[export]{adjustbox}
\DeclareMathOperator*{\argmin}{argmin} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage{lipsum}



\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{center}
    \begin{minipage}{10cm}
    	\begin{center}
    	\textbf{\large Project 4}\\[0.1cm]
        \textbf{Inesh Chakrabarti, Lawrence Liu, Nathan Wei}\\[0.1cm]
    	\end{center}
    \end{minipage}\hfill
\end{center}

\rule{17cm}{0.1mm}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section*{Introduction}
For the first part of this project we will do regression analysis. The dataset 
we chose to use is one of diamond characteristics. We will conduct regressions
to predict the price of a diamond given some features. 


\section*{Dataset}
Let us begin by understanding the dataset. The dataset consists of information
about 53940 round-cut diamonds with ten features: 
\begin{table}[ht]

\label{table1} 
\begin{tabular}{cl} 
\hline
\multicolumn{1}{c}{Feature} & \multicolumn{1}{c}{Description}\\
\hline 
    \texttt{carat} & weight of the diamond (0.2–5.01) \\
    \texttt{cut} & quality of the cut (Fair, Good, Very Good, Premium, Ideal) \\
    \texttt{color} & diamond colour, from J (worst) to D (best) \\
    \texttt{clarity} & a measurement of how clear the diamond is (I1 (worst),  
    SI2, SI1, VS2, VS1, VVS2, VVS1, IF (best)) \\
    \texttt{x} & length in mm (0–10.74) \\
    \texttt{y} & width in mm (0–58.9) \\
    \texttt{z} & depth in mm (0–31.8) \\
    \texttt{depth} & total depth percentage \\
    \texttt{table} & width of top of diamond relative to widest point (43-95) \\
    \texttt{price} & price in US dollars (\$326-\$18,823)
\end{tabular}
\end{table}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection*{Question 1.1}
\begin{figure}[H]
    \centering
   \includegraphics[width=0.5\linewidth]{../Figures/Question-1/datasetCorrHeatmap.png}
   \caption{Feature Pearson Correlation Heatmap}
   \label{fig:corr_hm}
\end{figure}

We will be using the nine features to predict the \texttt{price}. We can begin by 
computing the Pearson correlation matrix heatmap for these features in 
the dataset in Figure 1. Begin by noting that we assigned quantitative values to
the qualitative labels \texttt{cut},\texttt{color},\texttt{clarity} as ascending 
natural numbers based on ideality.\\

\begin{table}[H]
    \begin{subtable}[H]{0.45\textwidth}
        \begin{tabular}{c l}
            \hline
            Feature & \multicolumn{1}{c}{Correlation}         \\
            \hline 
        \texttt{carat}   & 0.9215914337868304    \\
        \texttt{cut}     & -0.05349263851362828  \\
        \texttt{color}   & -0.1725093772499559   \\
        \texttt{clarity} & -0.14680175361025616  \\
        \texttt{depth}   & -0.010647725608533299 \\
        \texttt{table}   & 0.12713358133531918   \\
        \texttt{x}       & 0.8844357793744166    \\
        \texttt{y}       & 0.865421694764742     \\
        \texttt{z}       & 0.861250266123968    
        \end{tabular}
        \caption{Price}
        \end{subtable}
        \begin{subtable}[H]{0.45\textwidth}
            \begin{tabular}{c l}
                \hline
                Feature & \multicolumn{1}{c}{Correlation}         \\
                \hline 
            \texttt{carat}   & 0.7694571626172851    \\
            \texttt{cut}     & 0.00542011950342582   \\
            \texttt{color}   & -0.011980043670033661 \\
            \texttt{clarity} & 0.04512538515850012   \\
            \texttt{depth}   & -0.03572374489729493  \\
            \texttt{table}   & 0.08458507638109278   \\
            \texttt{x}       & 0.7873455524189906    \\
            \texttt{y}       & 0.7717301198408058    \\
            \texttt{z}       & 0.7655421629234554   
            \end{tabular}
            \caption{Price per Carart}
        \end{subtable}
        \caption{Pearson Correlation Coefficients}
    \end{table}

The values for correlation for \texttt{price} is given in 
Table 1(a). We see that, unsurprisingly, there is a massive collection of high 
correlation squares at the bottom right. These indicate high Pearson 
correlation coefficient between \texttt{price} and \texttt{x}, \texttt{y}, 
\ texttt{z}. Similarily, there is also a high correlation with \texttt{carat}.
All of these suggest that the size of the diamond itself is the most significant 
predictor as to its price. \\

However, unexpectadly, we had a negative perason coeffciient for the quality of 
the \texttt{cut}, \texttt{color}, and \texttt{clarity}. This was odd, so we
similarily calculated the Pearson correlation values for Price per Carat instead, 
given in Table 1(b). We observed that the coefficient for \texttt{cut} and 
\texttt{clarity} became slightly positive, while color became close to zero. 
These seemed more in line with our expectations,and confirmed that the rarity of 
high carat and high clarity in a diamond at the same time was making the 
corresponding values in Table 1(a) negative. 

\subsubsection*{Question 1.2}
\begin{table}[H]
    \begin{subtable}[H]{0.45\textwidth}
    \centering
    \begin{tabular}{c l}
        \hline
        Feature & \multicolumn{1}{c}{Skewness} \\
        \hline
        \texttt{carat} & 1.116645920812613  \\ \
        \texttt{depth} & -0.08229402630189467 \\ \
        \texttt{table} &  0.7968958486695427\\ \
        \texttt{x} & 0.3786763426463927  \\ \
       \texttt{y} & 2.4341667164885554  \\ \
        \texttt{z} & 1.5224225590685583  \\ \
    \end{tabular}
    \caption{Before Processing}
    \end{subtable}
    \begin{subtable}[H]{0.45\textwidth}
        \centering
        \begin{tabular}{c c l}
            \hline
            Feature & Method & \multicolumn{1}{c}{Skewness} \\
            \hline
            \texttt{carat} & Box Cox & 0.020450070764268666 \\ \
            \texttt{depth} & No Change & -0.08229402630189467 \\ \
            \texttt{table} &  Box Cox & 0.020450070764268666\\ \
            \texttt{x} & No Change & 0.3786763426463927  \\ \
           \texttt{y} & Box Cox & 0.020450070764268666  \\ \
            \texttt{z} & Square Root & 0.0139742634447758  \\ \
        \end{tabular}
        \caption{After Processing}
        \end{subtable}
    \caption{Skewness of Features}
\end{table}

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../Figures/Question-1/featureprehist.png}
        \caption{Before Processing}
        \label{fig:clarityBox}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../Figures/Question-1/featureposthist.png}
        \caption{After Processing}
        \label{fig:colorBox}
    \end{subfigure}
       \caption{Histogram and KDE for Features}
       \label{fig:boxPlots}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Now, we examine the frequency distributions within each feature. We find that 
some of our numerical features are somewhat skewed—apparent from Table 2(a) and
Figure 2(a). We target a skewness less than $0.5$, as this would imply that the 
distribution is somewhat symetric. As such, we try three different methods: 
square-root, logrithm, and box-cox transformation. The first two are somewhat
self-explanatory; box-cox uses a non-zero value of $\lambda$ and conducts the
following transformation:
\begin{equation}
y_{\lambda}^{'} = \dfrac{y^{\lambda}-1}{\lambda \cdot \bar{g}_y^{\lambda-1}}
\end{equation}
where $\bar{g}_y$ is defined as the geometric mean of $y$. \\
We next try all of these methods, along with no transformatino, and take the 
minimum to try to minimize skewness in our data. The results are shown in Table 
2(b) and Figure 2(b).


\subsubsection*{Question 1.3}

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../Figures/Question-1/clarityBox.png}
        \caption{Clarity}
        \label{fig:clarityBox}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../Figures/Question-1/colorBox.png}
        \caption{Color}
        \label{fig:colorBox}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../Figures/Question-1/cutBox.png}
        \caption{Cut}
        \label{fig:cutBox}
    \end{subfigure}
       \caption{Categorical feature vs price boxplots}
       \label{fig:boxPlots}
\end{figure}

We see that idek what the fuck we see bro??

\subsubsection*{Question 2.1}
Now, before we train our regression models, we begin by splitting our data into 
training and testing sets. As such, we must now standardize the feature columns.
The function used to do this, \texttt{scaledTrainTest()}, as well as \texttt{
scaledTrainTestSplit()} can be found in \texttt{utils.py}.

\subsubsection*{Question 2.2}
Next, we begin with feature selection. We note that some of the features may not
be useful or may cause overfitting in our models as they do not carry useful 
useful information about the variable that we are trying to predict. To tell 
whether this is the case, we use two metrics: Mutual Information (MI) and F 
score.

\begin{figure}[H]
    \centering
   \includegraphics[width=0.8\linewidth]{../Figures/Question2/mi.png}
   \caption{Bar graph of F score and Mutual Information for features}
\end{figure}
\begin{table}[!ht]
    \centering
    \begin{tabular}{c l l}
    \hline
        Feature & Mutual Information & F Score \\ \hline
        \texttt{carat} & 1.65 & 273144 \\ \
        \texttt{cut} & 0.0578 & 139 \\ \
        \texttt{color} & 0.138 & 1465 \\ \
        \texttt{clarity} & 0.221 & 1079 \\ \
        \texttt{depth} & 0.0307 & 5.074 \\ \
        \texttt{table} & 0.0377 & 802 \\ \
        \texttt{x} & 1.41 & 174973 \\ \
       \texttt{y} & 1.42 & 142130 \\ \
        \texttt{z} & 1.36 & 138947 \\ \
    \end{tabular}
    \caption{Mutual Information and F Score Values for features}
\end{table}

It is clear by observation from Figure 3 and Table 2 that the lowest mutual 
information is present in \texttt{depth} and \texttt{table}. It is also 
important to note that the mutual information in cut is also very small. This 
makes sense as we saw earlier that the Pearson correlation coefficient for 
\texttt{cut} with respect to price per carat was almost zero. Similarily, the 
Pearson correlation coefficients for depth and table were very close to zero. \\

From this point, we will be testing the regression models without these three 
features, and with these three features and comparing the performance to 
determine the general performance.

DONT FORGET TO DO THIS!!!

\section*{Regression Models}
\subsection*{Linear Regression}
\subsubsection*{Question 4.1}
We begin with a simple regression, least square linear regression. Begin by 
noting the optimization problem:
\begin{equation}
    \argmin_{\theta} \dfrac{1}{2}||\textbf{Y}-\theta^T\hat{\textbf{X}}||^2
\end{equation}
Taking the derivative with respect to $\theta$ and setting it equal to zero 
gives us 
\begin{equation}
\theta = \left(\textbf{X}^T\textbf{X}\right)^{-1}\textbf{X}^T\textbf{Y}
\end{equation}
Now, we can add regularization terms, beginning with the simple L1 
regularization, also known in this case as a Lasso regression. This would make 
our new optimization problem:
\begin{equation}
    \argmin_{\theta} \dfrac{1}{2}||\textbf{Y}-\theta^T\hat{\textbf{X}}||^2 + 
    \alpha ||\theta||
\end{equation}
Note that in this case, our learned values for $\theta$ would become somewhat 
sparse as L1 regularization linearly penalizes any non-zero parameters.
As such, the gradient for the regularization term does not change until the 
parameter is zero, which leads to sparsity. However, in this case, as there 
aren't many terms, we would potenntially only have a few zero'd parameters in 
our predictive polynomial. \\

With L2 regularization, also known as a Ridge regression, we would have:
\begin{equation}
    \argmin_{\theta} \dfrac{1}{2}||\textbf{Y}-\theta^T\hat{\textbf{X}}||^2 + 
    \lambda ||\theta||^2
\end{equation}
Note in this case, we wouldn't have parameters that are zero; rather, we would 
have extremely small parameters. This is because the gradient for L2 
regularization decreases rapidly as the parameter itself becomes smaller and 
smaller due to the parabolic nature of the term. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




\end{document}

